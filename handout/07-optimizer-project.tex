\documentstyle[11pt,bnf,6035]{article}
\begin{document}

\handout{Optimizer Project Assignment}{\OPTIMIZERASSIGN}

\section*{Important Dates:}
{\bf Design Proposal: \OPTIMIZERDESIGNDUE}\\
{\bf Checkpoint: \OPTIMIZERCKPTDUE}\\
{\bf Final Compiler and Documentation: \OPTIMIZERDUE @ 11am}\\
{\bf Derby: \DERBY}\\

In this final phase of the project your goal is to reduce the
execution time of your generated code while preserving the semantics
of the given application.  This phase is completely opened-ended; you
are not required to implement any specific transformations.  Your
grade will be based on your overall design (with heavy emphasis on
the writeup) and how well your compiler performs as compared to the
submissions of the other groups.  During the final class, we will hold
the Compiler Derby where your compiler implementation will compete
against the implementations of your fellow classmates on a hidden
program (the derby program).

In the final month of class, we will cover various code improving
optimizations.  The AMD64 architecture is very complex and aggressive.
A substantial portion of this project phase is to determine which
optimizations will provide a benefit given the programs of the provided
benchmark suite and the target architecture. Some optimizations that
we will cover in class include:

\begin{itemize}

\item {\bf Register Allocation} -- Your compiler can implement a graph
coloring based register allocator.  See Chapter 16 of the Whale book
and \S 9.7 of the Dragon book. Your register allocator should take
advantage of the full set of general-purpose registers provided by the
x86-64 ISA.  It should also respect the Linux calling convention
including caller-save and callee-save properties of each register.

\item {\bf List Scheduling} --  Instruction scheduling minimizes
pipeline stalls for long latency operations such as loads, multiplies,
and divides.  See Chapter 17 of Whale book.

\item {\bf Instruction Selection} -- So far, we have been using a
restricted subset of the x86-64 ISA.  As a peephole optimization,
you might replace a sequence of simple instructions with a single,
complex instruction that performs the same operation in fewer
cycles.

\item {\bf Data Parallelization} -- Computation executed in
different iteration of a loop may be independent of each other. We
are providing you with a library to help you find independent
computations, and perform automatic loop parallelization. Since we
will run your code on a 4-way parallel machine, exploiting
parallelism may reduce your execution time by up to a factor of 4.
\end{itemize}

In order to identify and prioritize optimizations, we have provided
you with a benchmark suite of image-processing programs.  These
programs are more complex than the code that has been provided during
the previous phases, so your first priority is to produce correct
unoptimized assembly code for each benchmark.

After your compiler produces correct unoptimized code, you should
begin to study the applications of the benchmark suite.  You will use
these programs (and any other programs provided during previous
phases) to determine which optimizations you will implement and in
what order they will be applied.  You are expected to analyze the
assembly code produced by your compiler to classify the effectiveness
of each proposed optimization, perhaps first applying the optimization
manually and empirically measuring the benefit. Your writeup should
include evidence for the effectiveness of each optimization you
considered.

You are not limited to the optimizations covered in class.  You are
free to implement any optimization that you come across in your
reading or any optimization that you devise.  However, your writeup
must demonstrate that each optimization is effective and semantics
preserving.  Furthermore, you must argue that each optimization is
general, meaning it is not a special-case optimization that works only
for the derby program or a specific application in the benchmark suite.

You should consult AMD's documentation for details regarding our
target architecture and the ISA.  The documentation is linked from the
class website.  To benchmark your generated assembly code, provide the
{\tt -pg} option to {\tt gcc} or {\tt cc} while assembling in order to
generate profiling information.  After the code is executed, you can
use {\tt gprof} to examine the generated profile statistics.  Also,
you can use the Unix command {\tt time} focusing on the ``user''
time. A more accurate timing mechanism is included in the provided
thread library.  It will be discussed during the project recitation. 

\section*{Writeup}
The writeup for this project is extremely important. Although it
explicitly accounts for only 20\% of the grade, it will also be used
to determine your score for the Implementation aspect of the project
(40\%).

Your written documentation must discuss each optimization that you
considered.  The thoroughness of your exploration of the optimization
space will be an important aspect of your grade.  For each
optimization that you implement, your writeup must convince the reader
that the optimization was beneficial, general, and correct.  For each
optimization that you decide not to implement, you must convince the
reader that the optimization would not be beneficial given your
generated code for the given benchmarks and our target architecture.

You should include assembly or IR code examples for each
optimization. Show how your generated code is transformed by the
optimization (might be hand-applied for optimizations you did not
implement). Highlight the benefit of the optimization on the
assembly or IR code. Discuss exactly how the benefit was achieved
given the characteristics of our target architecture.  Include
empirical evidence that proves your conclusion for the optimization.

Your compiler should include a ``full optimizations'' command-line
option (see below).  Your written documentation should present a
detailed discussion of this option including how you determined the
order in which your optimizations are performed and how many times you
apply the optimizations.  Finally, describe
any hacks or solutions to tricky problems that you encountered.

\subsection*{Design Proposal}
As with previous phases, this phase includes a design proposal that
must be submitted.  This document will allow us to judge your progress.  We do not expect this document to be a
complete description of your implementation.  It should include a
detailed discussion of the optimizations that you have considered and
which optimizations are planned for implementation. The proposal
should include a discussion on the ``full optimizations'' option.

We expect your proposal to provide you with a substantiated plan of
action for implementation.  Of course, we are not ruling out the
implementation of optimizations not discussed in your proposal.

\section*{Derby}
On the final day of class we will hold the compiler derby.  Your
compiler will be pitted against the submissions from the other groups
in the class. The derby benchmark will be revealed one day prior to
the derby.  40\% of the grade will be determined by your ranking in
the derby.  At this time, we will not reveal the exact formula for
determining this portion of the project grade.

\section*{Parallelization}

We are providing you with a simple parallelization analysis library
to help you find parallelism in programs being compiled. The library
will help you compute the {\em distance vector}. For more
information you are encouraged to read \S 9.3 in the Whale book.
There will also be more information on specific implementation
details provided during the project information session.
You can find the library in {\tt
/mit/6.035/provided/optimizer/src/decaf/Parallel/ }. You may find
this library useful, or you may do your own analysis.

We will also provide you with a library that will help you spawn and
join threads. Again, you will be welcome to use it, or you can use
your own techniques for spawning and joining threads.

\section*{Compiler Submission}

Your compiler must provide a command-line option for each optimization.
Your project writeup should include documentation for each
command-line option.  For example:

\begin{itemize}

\item {\tt -opt regalloc}: turns on register allocation
\item {\tt -opt instsel}: turns on instructions selection peephole
optimizations
\item {\tt -opt listsched}: turns on list scheduling
\item {\tt -opt parallel}: turns on data parallelization of loops

\end{itemize}

You must provide a {\tt -opt all} flag to turn on all optimizations
and apply them in the order you have determined (``full
optimizations'').  This option should consider how many times each
optimization is applied and the application order of the
optimizations.

As before, your generated code must perform the runtime checks listed
in the language specification.  These may be optimized (or removed in
some cases) as long as they report a runtime error whenever the
unoptimized program reports an error.

\subsection*{What to Hand In}

Follow the directions given in project overview handout when writing up
your project.  For the written portion, make sure to include a description of
every optimization you implement.  Show clear {\em examples} of IR excerpts
(and x86-64 excerpts, if relevant) detailing what your optimizations do.
You should also include a discussion of how you determined the order in
which your optimizations are performed.

Each group must place their completed submission at:\[
\mbox{/mit/6.035/group/le{\bf NN}/submit/le{\bf NN}-optimizer.tar.gz}
\]

Submitted tarballs should have the following structure:
{\scriptsize
\begin{verbatim}
leNN-optimizer.tar.gz
|
`-- leNN-optimizer
    |
    |-- AUTHORS            (list of students in your group, one per line)
    |
    |-- code
    |   |
    |   ...                (full source code, can build by running `ant`)
    |
    |-- doc 
    |   |
    |   ...                (write-up, described in project overview handout)
    |   
    `-- dist 
        |
        `-- Compiler.jar   (compiled output, for automated testing)
\end{verbatim}
}

You should be able to run your compiler from the command line with:
\begin{verbatim}
java -jar dist/Compiler.jar -target codegen <filename>          # no optimizations
java -jar dist/Compiler.jar -target codegen -opt all <filename> # all optimizations
\end{verbatim}
Your compiler should then write a x86-64 assembly listing to: {\tt <filename>.s}

\section*{Test Cases}

We have provided sample programs that perform image processing
and filtering.  The programs are located in {\tt
/mit/6.035/provided/optimizer/tests}, along with test images. These
programs must be linked against the library provided in {\tt
/mit/6.035/provided/optimizer/lib}. You should make sure that any
valid program provided during previous phases continues to run
correctly.  We will also test your compiler (including
optimizations) on a suite of hidden programs.

\end{document}
